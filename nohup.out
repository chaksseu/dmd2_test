/home/work/anaconda3/envs/dmd2/lib/python3.8/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=False)
  warnings.warn(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: no

wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
/home/work/anaconda3/envs/dmd2/lib/python3.8/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=False)
  warnings.warn(
Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: no

wandb: Currently logged in as: rtrt505. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in ./mnt/localssd/log_stable_diffusion_coco/wandb/run-20241102_133506-ziyawypb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-shadow-61
wandb: ‚≠êÔ∏è View project at https://wandb.ai/rtrt505/DMD2_test
wandb: üöÄ View run at https://wandb.ai/rtrt505/DMD2_test/runs/ziyawypb
run dir: ./mnt/localssd/log_stable_diffusion_coco/wandb/run-20241102_133506-ziyawypb/files
/home/work/anaconda3/envs/dmd2/lib/python3.8/site-packages/diffusers/configuration_utils.py:140: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDIMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDIMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.
  deprecate("direct config name access", "1.0.0", deprecation_message, standard_warn=False)
/home/work/anaconda3/envs/dmd2/lib/python3.8/site-packages/diffusers/configuration_utils.py:140: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDIMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDIMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.
  deprecate("direct config name access", "1.0.0", deprecation_message, standard_warn=False)
Loaded 3023571 promptsLoaded 3023571 prompts

Dataset length: 480000
Dataset length: 480000
/home/work/anaconda3/envs/dmd2/lib/python3.8/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/home/work/anaconda3/envs/dmd2/lib/python3.8/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
start saving checkpoint to /home/work/StableDiffusion/DMD2/ckpt_path/laion6.25_sd_baseline_8node_guidance1.75_lr1e-5_seed10_dfake10_from_scratch/time_1730554505_seed10/checkpoint_model_000000
done saving
/home/work/anaconda3/envs/dmd2/lib/python3.8/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
W1102 13:41:07.251493 140213498836800 torch/distributed/elastic/agent/server/api.py:688] Received Signals.SIGHUP death signal, shutting down workers
W1102 13:41:07.253309 140213498836800 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 34665 closing signal SIGHUP
W1102 13:41:07.253674 140213498836800 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 34666 closing signal SIGHUP
W1102 13:41:37.254371 140213498836800 torch/distributed/elastic/multiprocessing/api.py:875] Unable to shutdown process 34665 via Signals.SIGHUP, forcefully exiting via Signals.SIGKILL
W1102 13:41:37.779957 140213498836800 torch/distributed/elastic/multiprocessing/api.py:875] Unable to shutdown process 34666 via Signals.SIGHUP, forcefully exiting via Signals.SIGKILL
Traceback (most recent call last):
  File "/home/work/anaconda3/envs/dmd2/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.4.1', 'console_scripts', 'torchrun')())
  File "/home/work/anaconda3/envs/dmd2/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/work/anaconda3/envs/dmd2/lib/python3.8/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/work/anaconda3/envs/dmd2/lib/python3.8/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/work/anaconda3/envs/dmd2/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/work/anaconda3/envs/dmd2/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/work/anaconda3/envs/dmd2/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/work/anaconda3/envs/dmd2/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/work/anaconda3/envs/dmd2/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 835, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/work/anaconda3/envs/dmd2/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 79, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 34641 got signal: 1
